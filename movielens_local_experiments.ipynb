{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MovieLens Recommendation System Experiments\n",
        "\n",
        "This notebook compares sequential (Surprise SVD) and distributed (Spark ALS) recommendation approaches on the MovieLens dataset.\n",
        "\n",
        "## Objectives\n",
        "1. Load and preprocess MovieLens dataset\n",
        "2. Implement sequential baseline using Surprise SVD\n",
        "3. Implement distributed model using Spark ALS\n",
        "4. Compare performance metrics and scalability\n",
        "5. Visualize results and generate insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Surprise library for collaborative filtering\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# PySpark for distributed computing\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, mean, stddev\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Loading and Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_movielens_data(data_path='./Dataset/'):\n",
        "    \"\"\"\n",
        "    Load MovieLens dataset files and return as pandas DataFrames\n",
        "    \"\"\"\n",
        "    print(\"Loading MovieLens dataset...\")\n",
        "    \n",
        "    # Load datasets\n",
        "    ratings = pd.read_csv(os.path.join(data_path, 'ratings.csv'))\n",
        "    movies = pd.read_csv(os.path.join(data_path, 'movies.csv'))\n",
        "    tags = pd.read_csv(os.path.join(data_path, 'tags.csv'))\n",
        "    links = pd.read_csv(os.path.join(data_path, 'links.csv'))\n",
        "    \n",
        "    return ratings, movies, tags, links\n",
        "\n",
        "def print_dataset_statistics(ratings, movies, tags, links):\n",
        "    \"\"\"\n",
        "    Print comprehensive statistics about the dataset\n",
        "    \"\"\"\n",
        "    print(\"\\n=== DATASET STATISTICS ===\")\n",
        "    print(f\"Ratings: {len(ratings):,} rows\")\n",
        "    print(f\"Movies: {len(movies):,} rows\")\n",
        "    print(f\"Tags: {len(tags):,} rows\")\n",
        "    print(f\"Links: {len(links):,} rows\")\n",
        "    \n",
        "    print(f\"\\nUnique users: {ratings['userId'].nunique():,}\")\n",
        "    print(f\"Unique movies: {ratings['movieId'].nunique():,}\")\n",
        "    print(f\"Unique tags: {tags['tag'].nunique() if len(tags) > 0 else 0:,}\")\n",
        "    \n",
        "    print(f\"\\nRating range: {ratings['rating'].min():.1f} - {ratings['rating'].max():.1f}\")\n",
        "    print(f\"Average rating: {ratings['rating'].mean():.2f}\")\n",
        "    print(f\"Rating std: {ratings['rating'].std():.2f}\")\n",
        "    \n",
        "    print(f\"\\nAverage ratings per user: {len(ratings) / ratings['userId'].nunique():.1f}\")\n",
        "    print(f\"Average ratings per movie: {len(ratings) / ratings['movieId'].nunique():.1f}\")\n",
        "    \n",
        "    # Rating distribution\n",
        "    print(\"\\nRating distribution:\")\n",
        "    rating_counts = ratings['rating'].value_counts().sort_index()\n",
        "    for rating, count in rating_counts.items():\n",
        "        print(f\"  {rating}: {count:,} ({count/len(ratings)*100:.1f}%)\")\n",
        "\n",
        "# Load the data\n",
        "ratings, movies, tags, links = load_movielens_data()\n",
        "print_dataset_statistics(ratings, movies, tags, links)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing and Splitting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stratified_train_test_split(ratings, test_size=0.2, min_ratings=5):\n",
        "    \"\"\"\n",
        "    Perform stratified train/test split per user\n",
        "    Only include users with at least min_ratings ratings\n",
        "    \"\"\"\n",
        "    print(f\"\\nPerforming stratified {int((1-test_size)*100)}/{int(test_size*100)} train/test split...\")\n",
        "    \n",
        "    # Filter users with sufficient ratings\n",
        "    user_counts = ratings['userId'].value_counts()\n",
        "    valid_users = user_counts[user_counts >= min_ratings].index\n",
        "    filtered_ratings = ratings[ratings['userId'].isin(valid_users)].copy()\n",
        "    \n",
        "    print(f\"Filtered to {len(valid_users):,} users with >= {min_ratings} ratings\")\n",
        "    print(f\"Remaining ratings: {len(filtered_ratings):,}\")\n",
        "    \n",
        "    train_data = []\n",
        "    test_data = []\n",
        "    \n",
        "    for user_id in tqdm(valid_users, desc=\"Splitting users\"):\n",
        "        user_ratings = filtered_ratings[filtered_ratings['userId'] == user_id]\n",
        "        \n",
        "        if len(user_ratings) >= min_ratings:\n",
        "            # Sort by timestamp to maintain temporal order\n",
        "            user_ratings = user_ratings.sort_values('timestamp')\n",
        "            \n",
        "            # Split maintaining chronological order\n",
        "            n_test = max(1, int(len(user_ratings) * test_size))\n",
        "            \n",
        "            train_ratings = user_ratings[:-n_test]\n",
        "            test_ratings = user_ratings[-n_test:]\n",
        "            \n",
        "            train_data.append(train_ratings)\n",
        "            test_data.append(test_ratings)\n",
        "    \n",
        "    train_df = pd.concat(train_data, ignore_index=True)\n",
        "    test_df = pd.concat(test_data, ignore_index=True)\n",
        "    \n",
        "    print(f\"\\nTrain set: {len(train_df):,} ratings\")\n",
        "    print(f\"Test set: {len(test_df):,} ratings\")\n",
        "    print(f\"Split ratio: {len(train_df)/(len(train_df)+len(test_df))*100:.1f}% / {len(test_df)/(len(train_df)+len(test_df))*100:.1f}%\")\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "def create_sample_split(train_df, test_df, sample_size=10000):\n",
        "    \"\"\"\n",
        "    Create a small sample for quick testing\n",
        "    \"\"\"\n",
        "    print(f\"\\nCreating sample split with {sample_size:,} training ratings...\")\n",
        "    \n",
        "    # Sample users proportionally\n",
        "    sample_train = train_df.sample(n=min(sample_size, len(train_df)), random_state=42)\n",
        "    sample_users = sample_train['userId'].unique()\n",
        "    \n",
        "    # Get corresponding test data for sampled users\n",
        "    sample_test = test_df[test_df['userId'].isin(sample_users)]\n",
        "    \n",
        "    print(f\"Sample train: {len(sample_train):,} ratings, {len(sample_users):,} users\")\n",
        "    print(f\"Sample test: {len(sample_test):,} ratings\")\n",
        "    \n",
        "    return sample_train, sample_test\n",
        "\n",
        "# Perform the splits\n",
        "train_df, test_df = stratified_train_test_split(ratings)\n",
        "sample_train, sample_test = create_sample_split(train_df, test_df)\n",
        "\n",
        "# Save splits to CSV files\n",
        "print(\"\\nSaving splits to CSV files...\")\n",
        "train_df.to_csv('./raw/train.csv', index=False)\n",
        "test_df.to_csv('./raw/test.csv', index=False)\n",
        "sample_train.to_csv('./raw/sample_train.csv', index=False)\n",
        "sample_test.to_csv('./raw/sample_test.csv', index=False)\n",
        "print(\"Splits saved successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
